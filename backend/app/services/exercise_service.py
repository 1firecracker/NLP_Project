"""æ ·æœ¬è¯•é¢˜æœåŠ¡ï¼Œå¤„ç†æ ·æœ¬è¯•é¢˜ä¸Šä¼ ã€è§£æã€ç®¡ç†"""
import base64
import json
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from uuid import uuid4

from fastapi import UploadFile

import app.config as config
from app.utils.exercise_parser import ExerciseParser

from app.agents.quiz_graph import run_agent_chain, validate_outputs
from app.agents.database.question_bank_storage import load_question_bank
from app.agents.shared_state import shared_state
from app.agents.agent_g_grader import run_agent_g
from app.agents.agent_g_grader import run_grade_student_submission
from app.agents.agent_h_learning_advisor import run_agent_h


class ExerciseService:
    """æ ·æœ¬è¯•é¢˜æœåŠ¡"""
    
    def __init__(self):
        self.parser = ExerciseParser()
        self.exercises_dir = Path(config.settings.exercises_dir)
        self.exercises_dir.mkdir(parents=True, exist_ok=True)
    
    def _get_conversation_samples_dir(self, conversation_id: str) -> Path:
        """è·å–å¯¹è¯çš„æ ·æœ¬è¯•é¢˜ç›®å½•"""
        return self.exercises_dir / conversation_id / "samples"
    
    def _get_sample_dir(self, conversation_id: str, sample_id: str) -> Path:
        """è·å–æ ·æœ¬è¯•é¢˜ç›®å½•"""
        return self._get_conversation_samples_dir(conversation_id) / sample_id
    
    def _get_metadata_file(self, conversation_id: str, sample_id: str) -> Path:
        """è·å–å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„"""
        return self._get_sample_dir(conversation_id, sample_id) / "metadata.json"
    
    def _validate_file(self, filename: str) -> tuple[bool, Optional[str]]:
        """éªŒè¯æ–‡ä»¶ç±»å‹"""
        file_ext = Path(filename).suffix.lower().lstrip('.')
        if file_ext not in config.settings.exercise_allowed_extensions:
            return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}ï¼Œä»…æ”¯æŒ {', '.join(config.settings.exercise_allowed_extensions)}"
        return True, None
    
    async def _check_file_size(self, file_content: bytes) -> tuple[bool, Optional[str]]:
        """æ£€æŸ¥æ–‡ä»¶å¤§å°"""
        file_size = len(file_content)
        if file_size > config.settings.max_file_size:
            return False, f"æ–‡ä»¶å¤§å° {file_size / 1024 / 1024:.2f}MB è¶…è¿‡é™åˆ¶ {config.settings.max_file_size / 1024 / 1024}MB"
        return True, None
    
    async def upload_samples(
        self,
        conversation_id: str,
        files: List[UploadFile]
    ) -> Dict:
        """ä¸Šä¼ æ ·æœ¬è¯•é¢˜
        
        Args:
            conversation_id: å¯¹è¯ID
            files: æ–‡ä»¶åˆ—è¡¨
            
        Returns:
            ä¸Šä¼ ç»“æœå­—å…¸
        """
        if not files:
            raise ValueError("è‡³å°‘éœ€è¦ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶")
        
        # æ£€æŸ¥æ ·æœ¬æ•°é‡é™åˆ¶
        existing_samples = self.list_samples(conversation_id)
        if len(existing_samples) + len(files) > config.settings.max_samples_per_conversation:
            raise ValueError(
                f"å½“å‰å·²æœ‰ {len(existing_samples)} ä¸ªæ ·æœ¬ï¼Œå†ä¸Šä¼  {len(files)} ä¸ªå°†è¶…è¿‡é™åˆ¶ "
                f"({config.settings.max_samples_per_conversation} ä¸ª)"
            )
        
        uploaded_samples = []
        samples_dir = self._get_conversation_samples_dir(conversation_id)
        samples_dir.mkdir(parents=True, exist_ok=True)
        
        for file in files:
            # éªŒè¯æ–‡ä»¶ç±»å‹
            is_valid, error_msg = self._validate_file(file.filename)
            if not is_valid:
                raise ValueError(error_msg)
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            file_content = await file.read()
            
            # éªŒè¯æ–‡ä»¶å¤§å°
            is_valid, error_msg = await self._check_file_size(file_content)
            if not is_valid:
                raise ValueError(error_msg)
            
            # ç”Ÿæˆæ ·æœ¬IDï¼ˆä½¿ç”¨æ–‡ä»¶åï¼Œå»é™¤æ‰©å±•åï¼‰
            original_path = Path(file.filename)
            sample_id = original_path.stem
            
            # å¦‚æœå·²å­˜åœ¨åŒåæ ·æœ¬ï¼Œæ·»åŠ åç¼€
            sample_dir = samples_dir / sample_id
            counter = 1
            while sample_dir.exists():
                sample_id = f"{original_path.stem}_{counter}"
                sample_dir = samples_dir / sample_id
                counter += 1
            
            # åˆ›å»ºæ ·æœ¬ç›®å½•
            sample_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜åŸå§‹æ–‡ä»¶
            original_file_path = sample_dir / file.filename
            with open(original_file_path, 'wb') as f:
                f.write(file_content)
            
            # åˆ›å»ºåˆå§‹å…ƒæ•°æ®ï¼ˆçŠ¶æ€ä¸º pendingï¼‰
            upload_time = datetime.utcnow().isoformat() + "Z"
            initial_metadata = {
                "sample_id": sample_id,
                "conversation_id": conversation_id,
                "original_filename": file.filename,
                "original_file_path": str(original_file_path.relative_to(sample_dir)),
                "file_type": Path(file.filename).suffix.lower().lstrip('.'),
                "file_size": len(file_content),
                "upload_time": upload_time,
                "status": "pending",
                "parse_start_time": None,
                "parse_end_time": None,
                "error": None
            }
            
            # ä¿å­˜åˆå§‹å…ƒæ•°æ®
            metadata_file = self._get_metadata_file(conversation_id, sample_id)
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(initial_metadata, f, ensure_ascii=False, indent=2)
            
            uploaded_samples.append({
                "sample_id": sample_id,
                "filename": file.filename,
                "file_size": len(file_content),
                "file_type": initial_metadata["file_type"],
                "status": "pending",
                "upload_time": upload_time
            })
        
        return {
            "conversation_id": conversation_id,
            "uploaded_samples": uploaded_samples,
            "total_samples": len(uploaded_samples)
        }
    
    def _parse_sample_async(
        self,
        conversation_id: str,
        sample_id: str,
        original_file_path: Path
    ):
        """å¼‚æ­¥è§£ææ ·æœ¬æ–‡ä»¶ï¼ˆåå°ä»»åŠ¡ï¼‰
        
        Args:
            conversation_id: å¯¹è¯ID
            sample_id: æ ·æœ¬ID
            original_file_path: åŸå§‹æ–‡ä»¶è·¯å¾„
        """
        import logging
        logger = logging.getLogger(__name__)
        
        metadata_file = self._get_metadata_file(conversation_id, sample_id)
        sample_dir = self._get_sample_dir(conversation_id, sample_id)
        
        try:
            # æ›´æ–°çŠ¶æ€ä¸º processing
            self._update_parse_status(
                conversation_id, sample_id,
                status="processing",
                parse_start_time=datetime.utcnow().isoformat() + "Z"
            )
            
            # ä½¿ç”¨ExerciseParserè§£ææ–‡ä»¶
            content = self.parser.extract_content(
                str(original_file_path),
                save_to=None,
                conversation_id=conversation_id,
                document_id=sample_id,
            )
            
            # ç›´æ¥ä¿å­˜æ–‡æœ¬å’Œå›¾ç‰‡åˆ° sample_dir
            text_file = sample_dir / "text.txt"
            text_file.write_text(content["text"], encoding='utf-8')
            
            # ä¿å­˜å›¾ç‰‡
            images_dir = sample_dir / "images"
            images_dir.mkdir(exist_ok=True)
            
            saved_images = []
            for i, img in enumerate(content.get("images", [])):
                try:
                    # æƒ…å†µ1ï¼šæœ‰ image_base64ï¼ˆæœ¬åœ°è§£æï¼Œå¦‚ PyMuPDFï¼‰
                    if img.get("image_base64"):
                        img_data = base64.b64decode(img["image_base64"])
                        img_index = img.get("image_index", i + 1)
                        img_format = img.get("image_format", "png")
                        page_number = img.get("page_number", 1)
                        img_file = images_dir / f"page_{page_number}_img_{img_index}.{img_format}"
                        img_file.write_bytes(img_data)
                        
                        logger.info(f"ä¿å­˜å›¾ç‰‡æˆåŠŸ: {img_file.name}, å¤§å°: {len(img_data)} å­—èŠ‚")
                        
                        saved_images.append({
                            "page_number": page_number,
                            "image_index": img_index,
                            "file_path": f"images/{img_file.name}",
                            "image_format": img_format,
                            "width": img.get("width", 0),
                            "height": img.get("height", 0)
                        })
                    # æƒ…å†µ2ï¼šåªæœ‰ file_pathï¼ˆGitee OCRï¼Œå›¾ç‰‡å·²ä¿å­˜ï¼‰
                    elif img.get("file_path"):
                        file_path = img.get("file_path")
                        # file_path å¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼ˆå¦‚ "images/image_4_1.jpg"ï¼‰æˆ–ç»å¯¹è·¯å¾„
                        if file_path.startswith("images/"):
                            # ç›¸å¯¹è·¯å¾„ï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                            img_file = sample_dir / file_path
                            if img_file.exists():
                                logger.info(f"ä½¿ç”¨å·²å­˜åœ¨çš„å›¾ç‰‡: {file_path}")
                                saved_images.append({
                                    "page_number": img.get("page_number"),
                                    "image_index": img.get("image_index", i + 1),
                                    "file_path": file_path,
                                    "image_format": img.get("image_format", img_file.suffix.lstrip('.')),
                                    "width": img.get("width", 0),
                                    "height": img.get("height", 0)
                                })
                            else:
                                logger.warning(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                        else:
                            # ç»å¯¹è·¯å¾„æˆ–å…¶ä»–æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
                            logger.info(f"ä½¿ç”¨å›¾ç‰‡è·¯å¾„: {file_path}")
                            saved_images.append({
                                "page_number": img.get("page_number"),
                                "image_index": img.get("image_index", i + 1),
                                "file_path": file_path,
                                "image_format": img.get("image_format", "unknown"),
                                "width": img.get("width", 0),
                                "height": img.get("height", 0)
                            })
                    else:
                        logger.warning(f"å›¾ç‰‡ {i+1} æ—¢æ²¡æœ‰ base64 æ•°æ®ä¹Ÿæ²¡æœ‰ file_pathï¼Œè·³è¿‡")
                        continue
                except Exception as e:
                    logger.error(f"å¤„ç†å›¾ç‰‡ {i+1} å¤±è´¥: {e}", exc_info=True)
                    continue
            
            # æ›´æ–°å…ƒæ•°æ®ï¼ˆè§£ææˆåŠŸï¼‰
            parse_end_time = datetime.utcnow().isoformat() + "Z"
            self._update_parse_status(
                conversation_id, sample_id,
                status="completed",
                parse_end_time=parse_end_time,
                file_type=content["file_type"],
                text_length=len(content["text"]),
                image_count=len(saved_images),
                images=saved_images
            )
            
            logger.info(f"æ ·æœ¬è§£æå®Œæˆ: sample_id={sample_id}, image_count={len(saved_images)}")
            
        except Exception as e:
            # æ›´æ–°çŠ¶æ€ä¸º failed
            error_msg = str(e)
            logger.error(f"æ ·æœ¬è§£æå¤±è´¥: sample_id={sample_id}, é”™è¯¯: {error_msg}", exc_info=True)
            self._update_parse_status(
                conversation_id, sample_id,
                status="failed",
                parse_end_time=datetime.utcnow().isoformat() + "Z",
                error=error_msg
            )
    
    def _update_parse_status(
        self,
        conversation_id: str,
        sample_id: str,
        status: str,
        parse_start_time: Optional[str] = None,
        parse_end_time: Optional[str] = None,
        error: Optional[str] = None,
        file_type: Optional[str] = None,
        text_length: Optional[int] = None,
        image_count: Optional[int] = None,
        images: Optional[List[Dict]] = None
    ):
        """æ›´æ–°è§£æçŠ¶æ€
        
        Args:
            conversation_id: å¯¹è¯ID
            sample_id: æ ·æœ¬ID
            status: çŠ¶æ€ (pending/processing/completed/failed)
            parse_start_time: è§£æå¼€å§‹æ—¶é—´
            parse_end_time: è§£æç»“æŸæ—¶é—´
            error: é”™è¯¯ä¿¡æ¯
            file_type: æ–‡ä»¶ç±»å‹
            text_length: æ–‡æœ¬é•¿åº¦
            image_count: å›¾ç‰‡æ•°é‡
            images: å›¾ç‰‡åˆ—è¡¨
        """
        metadata_file = self._get_metadata_file(conversation_id, sample_id)
        
        if not metadata_file.exists():
            return
        
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            metadata["status"] = status
            if parse_start_time is not None:
                metadata["parse_start_time"] = parse_start_time
            if parse_end_time is not None:
                metadata["parse_end_time"] = parse_end_time
            if error is not None:
                metadata["error"] = error
            if file_type is not None:
                metadata["file_type"] = file_type
            if text_length is not None:
                metadata["text_length"] = text_length
                metadata["text_file"] = "text.txt"
            if image_count is not None:
                metadata["image_count"] = image_count
            if images is not None:
                metadata["images"] = images
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            import logging
            logger = logging.getLogger(__name__)
            logger.error(f"æ›´æ–°è§£æçŠ¶æ€å¤±è´¥: {e}", exc_info=True)
    
    def list_samples(self, conversation_id: str) -> List[Dict]:
        """è·å–æ ·æœ¬è¯•é¢˜åˆ—è¡¨
        
        Args:
            conversation_id: å¯¹è¯ID
            
        Returns:
            æ ·æœ¬è¯•é¢˜åˆ—è¡¨
        """
        samples_dir = self._get_conversation_samples_dir(conversation_id)
        if not samples_dir.exists():
            return []
        
        samples = []
        for sample_dir in samples_dir.iterdir():
            if not sample_dir.is_dir():
                continue
            
            metadata_file = sample_dir / "metadata.json"
            if not metadata_file.exists():
                continue
            
            try:
                with open(metadata_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    
                    # è·å–å›¾ç‰‡æ•°é‡ï¼šä¼˜å…ˆä½¿ç”¨ metadata ä¸­çš„å€¼ï¼Œå¦‚æœä¸º 0 åˆ™ä»æ–‡ä»¶ç³»ç»Ÿæ£€æŸ¥
                    image_count = metadata.get("image_count", 0)
                    if image_count == 0:
                        # æ£€æŸ¥ images æ•°ç»„
                        images_list = metadata.get("images", [])
                        if images_list:
                            image_count = len(images_list)
                        else:
                            # ä»æ–‡ä»¶ç³»ç»Ÿæ£€æŸ¥å®é™…å›¾ç‰‡æ–‡ä»¶æ•°é‡
                            images_dir = sample_dir / "images"
                            if images_dir.exists() and images_dir.is_dir():
                                image_files = [f for f in images_dir.iterdir() if f.is_file() and f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.webp']]
                                image_count = len(image_files)
                    
                    samples.append({
                        "sample_id": metadata.get("sample_id", sample_dir.name),
                        "filename": metadata.get("original_filename", metadata.get("filename", sample_dir.name)),
                        "file_type": metadata.get("file_type", "unknown"),
                        "file_size": metadata.get("file_size", 0),
                        "text_length": metadata.get("text_length", 0),
                        "image_count": image_count,
                        "upload_time": metadata.get("upload_time", ""),
                        "status": metadata.get("status", "unknown")
                    })
            except Exception:
                continue
        
        # æŒ‰ä¸Šä¼ æ—¶é—´å€’åºæ’åˆ—
        samples.sort(key=lambda x: x.get("upload_time", ""), reverse=True)
        return samples
    
    def get_sample(self, conversation_id: str, sample_id: str) -> Optional[Dict]:
        """è·å–æ ·æœ¬è¯•é¢˜è¯¦æƒ…
        
        Args:
            conversation_id: å¯¹è¯ID
            sample_id: æ ·æœ¬ID
            
        Returns:
            æ ·æœ¬è¯•é¢˜è¯¦æƒ…ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        metadata_file = self._get_metadata_file(conversation_id, sample_id)
        if not metadata_file.exists():
            return None
        
        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # ç¡®ä¿åŒ…å«å¿…è¦å­—æ®µï¼ˆå…¼å®¹æ—§æ ¼å¼ï¼‰
            if "sample_id" not in metadata:
                metadata["sample_id"] = sample_id
            if "conversation_id" not in metadata:
                metadata["conversation_id"] = conversation_id
            if "filename" not in metadata:
                # å…¼å®¹æ—§æ ¼å¼ï¼šfile_id æˆ– original_filename
                metadata["filename"] = metadata.get("original_filename", metadata.get("file_id", sample_id))
            if "original_filename" not in metadata:
                metadata["original_filename"] = metadata.get("filename", sample_id)
            
            # ç¡®ä¿å¿…éœ€å­—æ®µæœ‰é»˜è®¤å€¼ï¼ˆå¿…é¡»åœ¨è¯»å–æ–‡æœ¬å‰è®¾ç½®ï¼‰
            metadata.setdefault("file_size", 0)
            metadata.setdefault("upload_time", "")
            metadata.setdefault("images", [])
            
            # è¯»å–æ–‡æœ¬å†…å®¹
            sample_dir = self._get_sample_dir(conversation_id, sample_id)
            text_file = sample_dir / metadata.get("text_file", "text.txt")
            text_content = ""
            
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨å­ç›®å½•ä¸­æŸ¥æ‰¾ï¼ˆå…¼å®¹æ—§çš„ä¿å­˜ç»“æ„ï¼‰
            if not text_file.exists():
                # æ£€æŸ¥æ˜¯å¦æœ‰å­ç›®å½•ï¼ˆæ—§æ ¼å¼ï¼šsample_dir/sample_id/text.txtï¼‰
                sub_dir = sample_dir / sample_id
                if sub_dir.exists() and sub_dir.is_dir():
                    alt_text_file = sub_dir / metadata.get("text_file", "text.txt")
                    if alt_text_file.exists():
                        text_file = alt_text_file
            
            if text_file.exists():
                try:
                    with open(text_file, 'r', encoding='utf-8') as f:
                        text_content = f.read()
                except Exception as e:
                    import logging
                    logger = logging.getLogger(__name__)
                    logger.error(f"è¯»å–æ–‡æœ¬æ–‡ä»¶å¤±è´¥: {text_file}, é”™è¯¯: {e}")
                    text_content = ""
            
            metadata["text_content"] = text_content
            return metadata
        except Exception:
            return None
    
    def delete_sample(self, conversation_id: str, sample_id: str) -> bool:
        """åˆ é™¤æ ·æœ¬è¯•é¢˜
        
        Args:
            conversation_id: å¯¹è¯ID
            sample_id: æ ·æœ¬ID
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        sample_dir = self._get_sample_dir(conversation_id, sample_id)
        if not sample_dir.exists():
            return False
        
        try:
            shutil.rmtree(sample_dir)
            return True
        except Exception:
            return False
    
    def get_sample_text(self, conversation_id: str, sample_id: str) -> Optional[str]:
        """è·å–æ ·æœ¬è¯•é¢˜æ–‡æœ¬å†…å®¹
        
        Args:
            conversation_id: å¯¹è¯ID
            sample_id: æ ·æœ¬ID
            
        Returns:
            æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        sample = self.get_sample(conversation_id, sample_id)
        if not sample:
            return None
        return sample.get("text_content", "")

    def _find_any_completed_conversation(self) -> Optional[str]:
        """
        åœ¨ exercises æ ¹ç›®å½•ä¸‹è‡ªåŠ¨å¯»æ‰¾â€œè‡³å°‘æœ‰ä¸€ä¸ªè§£æå®Œæˆæ ·æœ¬â€çš„ conversation_idï¼Œ
        è¿”å›æœ€è¿‘ä¸Šä¼ çš„é‚£ä¸ª conversation_idï¼›å¦‚æœæ‰¾ä¸åˆ°åˆ™è¿”å› Noneã€‚
        """
        if not self.exercises_dir.exists():
            return None

        candidates: list[tuple[str, str]] = []  # (latest_upload_time, conversation_id)

        for conv_dir in self.exercises_dir.iterdir():
            if not conv_dir.is_dir():
                continue
            conv_id = conv_dir.name
            samples = self.list_samples(conv_id)
            # åªè¦æœ‰ä¸€ä¸ªçŠ¶æ€ä¸º completed çš„æ ·æœ¬ï¼Œå°±è®¤ä¸ºè¿™ä¸ªä¼šè¯å¯ç”¨
            completed = [s for s in samples if s.get("status") == "completed"]
            if not completed:
                continue
            # æ‰¾è¿™ä¸ªä¼šè¯é‡Œæœ€æ–°çš„ä¸Šä¼ æ—¶é—´
            latest_time = max(s.get("upload_time", "") for s in completed)
            candidates.append((latest_time, conv_id))

        if not candidates:
            return None

        # æŒ‰ä¸Šä¼ æ—¶é—´é€†åºæ’åºï¼Œå–æœ€æ–°çš„é‚£ä¸ª conversation_id
        candidates.sort(key=lambda x: x[0], reverse=True)
        return candidates[0][1]


    def _clear_generated_cache(self, conversation_id: str):
        """
        æ¸…é™¤æŒ‡å®šä¼šè¯çš„ç”Ÿæˆé¢˜åº“ç¼“å­˜ï¼ˆç£ç›˜æ–‡ä»¶å’Œå†…å­˜çŠ¶æ€ï¼‰
        """
        import os
        from app.agents.database.question_bank_storage import BASE_DATA_DIR
        
        # æ¸…é™¤å†…å­˜ç¼“å­˜
        shared_state.reset()
        
        # æ¸…é™¤ç£ç›˜æ–‡ä»¶ï¼ˆåŒ…æ‹¬åŸå§‹é¢˜åº“å’Œæ‰€æœ‰ç”Ÿæˆé˜¶æ®µï¼‰
        suffixes = ['', '_generated', '_corrected', '_graded']
        for suffix in suffixes:
            cache_id = f"{conversation_id}{suffix}"
            cache_dir = os.path.join(BASE_DATA_DIR, cache_id)
            if os.path.exists(cache_dir):
                try:
                    shutil.rmtree(cache_dir)
                    print(f"[ğŸ—‘ï¸ å·²æ¸…é™¤ç¼“å­˜] {cache_dir}")
                except Exception as e:
                    print(f"[âš ï¸ æ¸…é™¤ç¼“å­˜å¤±è´¥] {cache_dir}: {e}")

    def generate_questions(self, conversation_id: str, up_to: str = "F") -> Dict:
        """
        åŸºäºå½“å‰/æœ€è¿‘ä¸€æ¬¡å·²ä¸Šä¼ å¹¶è§£æå®Œæˆçš„æ ·æœ¬è¯•é¢˜ï¼Œ
        å¯åŠ¨æ•´æ¡å‡ºé¢˜ Agent é“¾ï¼ˆA~Fï¼‰ï¼Œå¹¶è¿”å›ç”Ÿæˆç»“æœæ¦‚è¦ã€‚

        æ³¨æ„ï¼š
        - ä¸å†å¼ºä¾èµ–å‰ç«¯ä¼ å…¥çš„ conversation_idï¼›
        - å¦‚æœå½“å‰ conversation ä¸‹æ²¡æœ‰æ ·æœ¬ï¼Œä¼šè‡ªåŠ¨åœ¨æ‰€æœ‰ä¼šè¯ä¸­å¯»æ‰¾æœ€è¿‘çš„ä¸€ä¸ªã€‚
        """
        # åªä½¿ç”¨å½“å‰ conversation_idï¼Œä¸å†è‡ªåŠ¨æŸ¥æ‰¾å…¶ä»–ä¼šè¯
        effective_id = conversation_id
        samples = self.list_samples(conversation_id)

        # æ£€æŸ¥å½“å‰ä¼šè¯æ˜¯å¦æœ‰å·²å®Œæˆçš„æ ·æœ¬
        if not samples:
            raise ValueError(
                f"å½“å‰ä¼šè¯ [{conversation_id}] æœªæ‰¾åˆ°ä»»ä½•æ ·æœ¬è¯•å·ã€‚\n"
                "è¯·å…ˆåœ¨ã€æ ·æœ¬è¯•å·ã€‘æ¨¡å—ä¸Šä¼  PDF/DOCX/TXT æ–‡ä»¶ï¼Œå¹¶ç­‰å¾…è§£æå®Œæˆåå†ç”Ÿæˆè¯•é¢˜ã€‚"
            )
        
        completed_samples = [s for s in samples if s.get("status") == "completed"]
        if not completed_samples:
            pending_count = len([s for s in samples if s.get("status") == "pending"])
            if pending_count > 0:
                raise ValueError(
                    f"å½“å‰ä¼šè¯æœ‰ {pending_count} ä¸ªæ ·æœ¬æ­£åœ¨è§£æä¸­ï¼Œè¯·ç¨ç­‰ç‰‡åˆ»åå†ç”Ÿæˆè¯•é¢˜ã€‚"
                )
            else:
                raise ValueError(
                    f"å½“å‰ä¼šè¯çš„æ ·æœ¬è§£æå¤±è´¥ã€‚è¯·é‡æ–°ä¸Šä¼ æ ·æœ¬è¯•å·æˆ–æ£€æŸ¥æ–‡ä»¶æ ¼å¼ã€‚"
                )

        # æ¸…é™¤æ—§ç¼“å­˜ï¼ˆç¡®ä¿ç”Ÿæˆæ–°é¢˜ç›®ï¼‰
        print(f"[ğŸ”„ æ¸…é™¤æ—§ç¼“å­˜] ä¼šè¯ {effective_id}")
        self._clear_generated_cache(effective_id)
        
        print(f"[AgentPipeline] ä½¿ç”¨ä¼šè¯ {effective_id} ç”Ÿæˆæ–°é¢˜ç›®ï¼ˆæ‰¾åˆ° {len(completed_samples)} ä¸ªå·²å®Œæˆæ ·æœ¬ï¼‰")

        # 2) å¯åŠ¨ Agent é“¾
        #    Agent A ä¼šåœ¨ run_agent_a ä¸­ä½¿ç”¨ "__AUTO__" è‡ªåŠ¨æ‰«æ backend/uploads/exercises ä¸‹çš„ text.txt
        run_agent_chain(effective_id, ["__AUTO__"], up_to=up_to)

        # 3) ç®¡é“å¥åº·æ£€æŸ¥ï¼ˆA~F å“ªäº›æˆåŠŸ/ç¼ºå¤±ï¼‰
        pipeline_status = validate_outputs(effective_id)

        # 4) è½½å…¥ç”Ÿæˆåçš„é¢˜åº“ï¼ˆ<conversation_id>_generatedï¼‰
        generated_id = f"{effective_id}_generated"
        qb_generated = load_question_bank(generated_id)
        if qb_generated is None or not getattr(qb_generated, "questions", None):
            raise ValueError("é¢˜ç›®ç”Ÿæˆæµç¨‹å·²å®Œæˆï¼Œä½†æœªæ‰¾åˆ°ç”Ÿæˆé¢˜åº“æ–‡ä»¶ã€‚")

        question_count = len(qb_generated.questions)

        # 5) æ‹¿åˆ° Agent F çš„è´¨é‡æŠ¥å‘Šï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        quality_report = getattr(shared_state, "quality_report", None)

        return {
            "conversation_id": effective_id,
            "generated_conversation_id": generated_id,
            "question_count": question_count,
            "pipeline_status": pipeline_status,
            "quality_report": quality_report,
        }

    def grade_generated(self, conversation_id: str):
        """
        å¯¹å·²ç»ç”Ÿæˆçš„è¯•é¢˜ï¼ˆconversation_id_generatedï¼‰è¿è¡Œ Agent G æ‰¹æ”¹å™¨ã€‚
        è¿”å› quality_reportã€‚
        """
        # ä¸¥æ ¼ä½¿ç”¨å½“å‰ä¼šè¯ï¼Œä¸å†è‡ªåŠ¨æŸ¥æ‰¾å…¶ä»–ä¼šè¯
        generated_id = f"{conversation_id}_generated"
        qb = load_question_bank(generated_id)
        
        if qb is None or not getattr(qb, "questions", None):
            raise ValueError(
                f"å½“å‰ä¼šè¯ [{conversation_id}] æœªæ‰¾åˆ°ç”Ÿæˆçš„é¢˜åº“ã€‚\n"
                "è¯·å…ˆåœ¨ã€è¯•é¢˜ç”Ÿæˆã€‘æ¨¡å—ç‚¹å‡»'ç”Ÿæˆè¯•é¢˜'æŒ‰é’®ï¼Œç­‰å¾…ç”Ÿæˆå®Œæˆåå†æ‰¹æ”¹ã€‚"
            )

        # è°ƒç”¨ Agent Gï¼ˆåŒæ­¥åŒ…è£…ï¼‰
        report = run_agent_g(conversation_id)
        return report

    def grade_submission(self, conversation_id: str, student_name: str, answers_map: dict):
        """
        ä½¿ç”¨ Agent G å¯¹å­¦ç”Ÿæäº¤çš„ answers_mapï¼ˆ{questionId: answer}ï¼‰è¿›è¡Œè¯„åˆ†ã€‚
        ç„¶åè°ƒç”¨ Agent H ç”Ÿæˆå­¦ä¹ è¯Šæ–­ä¸å»ºè®®ã€‚
        æœ€åç”ŸæˆPDFæŠ¥å‘Šã€‚
        """
        # Call Agent G for grading (synchronous wrapper)
        report = run_grade_student_submission(conversation_id, student_name, answers_map)
        
        # Add timestamp for record tracking
        timestamp = datetime.now().isoformat()
        report["timestamp"] = timestamp
        
        # Call Agent H for learning advice
        try:
            h_report = run_agent_h(report, conversation_id, student_name)
            # Merge Agent H results into the grading report
            report["learning_advice"] = h_report
            # Enhance recommendations with Agent H's priority topics
            if h_report.get("learning_plan", {}).get("priority_topics"):
                priority_topics = h_report["learning_plan"]["priority_topics"]
                for pt in priority_topics:
                    report["recommendations"].append(
                        f"ä¼˜å…ˆå­¦ä¹ ï¼š{pt['topic']} - {pt['reason']}"
                    )
        except Exception as e:
            print(f"[âš ï¸ Agent H æ‰§è¡Œå¤±è´¥ï¼Œè·³è¿‡å­¦ä¹ å»ºè®®ç”Ÿæˆ] {e}")
            report["learning_advice"] = None
        
        # Generate PDF report
        try:
            pdf_path = self._generate_grading_pdf(conversation_id, student_name, report)
            report["pdf_path"] = pdf_path
            print(f"[âœ… PDFæŠ¥å‘Šå·²ç”Ÿæˆ] {pdf_path}")
        except Exception as e:
            print(f"[âš ï¸ PDFç”Ÿæˆå¤±è´¥] {e}")
            report["pdf_path"] = None
        
        return report

    def _generate_grading_pdf(self, conversation_id: str, student_name: str, report: dict) -> str:
        """
        ç”Ÿæˆæ‰¹æ”¹æŠ¥å‘ŠPDF
        è¿”å›PDFæ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„
        """
        from reportlab.lib.pagesizes import A4
        from reportlab.lib import colors
        from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
        from reportlab.platypus import SimpleDocTemplate, Table, TableStyle, Paragraph, Spacer, PageBreak
        from reportlab.pdfbase import pdfmetrics
        from reportlab.pdfbase.ttfonts import TTFont
        from reportlab.lib.units import cm
        
        # åˆ›å»ºPDFç›®å½•
        pdf_dir = Path(config.settings.data_dir) / conversation_id / "grading_reports"
        pdf_dir.mkdir(parents=True, exist_ok=True)
        
        # ç”ŸæˆPDFæ–‡ä»¶å
        timestamp_str = datetime.now().strftime('%Y%m%d%H%M%S')
        pdf_filename = f"grading_{student_name}_{timestamp_str}.pdf"
        pdf_path = pdf_dir / pdf_filename
        
        # æ³¨å†Œä¸­æ–‡å­—ä½“ï¼ˆä½¿ç”¨ç³»ç»Ÿå­—ä½“ï¼‰
        try:
            font_path = "C:/Windows/Fonts/msyh.ttc"  # å¾®è½¯é›…é»‘
            pdfmetrics.registerFont(TTFont('msyh', font_path))
            font_name = 'msyh'
        except:
            font_name = 'Helvetica'  # å›é€€åˆ°é»˜è®¤å­—ä½“
        
        # åˆ›å»ºPDFæ–‡æ¡£
        doc = SimpleDocTemplate(str(pdf_path), pagesize=A4)
        elements = []
        
        # æ ·å¼
        styles = getSampleStyleSheet()
        title_style = ParagraphStyle(
            'CustomTitle',
            parent=styles['Heading1'],
            fontName=font_name,
            fontSize=20,
            textColor=colors.HexColor('#1a73e8'),
            spaceAfter=20,
            alignment=1  # å±…ä¸­
        )
        heading_style = ParagraphStyle(
            'CustomHeading',
            parent=styles['Heading2'],
            fontName=font_name,
            fontSize=14,
            textColor=colors.HexColor('#333333'),
            spaceAfter=12
        )
        body_style = ParagraphStyle(
            'CustomBody',
            parent=styles['BodyText'],
            fontName=font_name,
            fontSize=10,
            textColor=colors.HexColor('#666666')
        )
        
        # æ ‡é¢˜
        elements.append(Paragraph("è¯•å·æ‰¹æ”¹æŠ¥å‘Š", title_style))
        elements.append(Spacer(1, 0.5*cm))
        
        # åŸºæœ¬ä¿¡æ¯
        info_data = [
            ["å­¦ç”Ÿå§“å", student_name or "åŒ¿å"],
            ["æ‰¹æ”¹æ—¶é—´", report.get("graded_at", "")[:19].replace("T", " ")],
            ["æ€»é¢˜æ•°", str(report.get("question_count", 0))],
            ["å¹³å‡åˆ†", f"{report.get('average_score', 0):.1f} / 100"]
        ]
        info_table = Table(info_data, colWidths=[4*cm, 12*cm])
        info_table.setStyle(TableStyle([
            ('FONTNAME', (0, 0), (-1, -1), font_name),
            ('FONTSIZE', (0, 0), (-1, -1), 10),
            ('TEXTCOLOR', (0, 0), (0, -1), colors.HexColor('#1a73e8')),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
            ('BACKGROUND', (0, 0), (0, -1), colors.HexColor('#f0f7ff')),
            ('PADDING', (0, 0), (-1, -1), 8)
        ]))
        elements.append(info_table)
        elements.append(Spacer(1, 0.8*cm))
        
        # é¢˜ç›®æ‰¹æ”¹è¯¦æƒ…
        elements.append(Paragraph("é¢˜ç›®æ‰¹æ”¹è¯¦æƒ…", heading_style))
        question_data = [["é¢˜å·", "é¢˜å‹", "å¾—åˆ†", "åé¦ˆ"]]
        for item in report.get("per_question", []):
            question_data.append([
                item.get("id", ""),
                item.get("question_type", ""),
                f"{item.get('score', 0)}/100",
                item.get("feedback", "")[:50] + "..." if len(item.get("feedback", "")) > 50 else item.get("feedback", "")
            ])
        
        question_table = Table(question_data, colWidths=[2*cm, 3*cm, 2.5*cm, 8.5*cm])
        question_table.setStyle(TableStyle([
            ('FONTNAME', (0, 0), (-1, -1), font_name),
            ('FONTSIZE', (0, 0), (-1, -1), 9),
            ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#1a73e8')),
            ('TEXTCOLOR', (0, 0), (-1, 0), colors.whitesmoke),
            ('GRID', (0, 0), (-1, -1), 0.5, colors.grey),
            ('PADDING', (0, 0), (-1, -1), 6),
            ('VALIGN', (0, 0), (-1, -1), 'MIDDLE')
        ]))
        elements.append(question_table)
        elements.append(Spacer(1, 0.8*cm))
        
        # å­¦ä¹ å»ºè®®
        if report.get("recommendations"):
            elements.append(Paragraph("å­¦ä¹ å»ºè®®", heading_style))
            for rec in report.get("recommendations", []):
                elements.append(Paragraph(f"â€¢ {rec}", body_style))
                elements.append(Spacer(1, 0.2*cm))
        
        # ç”ŸæˆPDF
        doc.build(elements)
        
        # è¿”å›ç›¸å¯¹è·¯å¾„ï¼ˆç›¸å¯¹äºdataç›®å½•ï¼‰
        return f"{conversation_id}/grading_reports/{pdf_filename}"

    def get_all_records(self, conversation_id: str) -> List[Dict]:
        """
        è·å–æŒ‡å®šä¼šè¯ä¸‹æ‰€æœ‰å­¦ç”Ÿçš„æ‰¹æ”¹è®°å½•
        ä» data/<conversation_id>/submissions/ ç›®å½•è¯»å–æ‰€æœ‰ JSON æ–‡ä»¶
        """
        submissions_dir = Path(config.settings.data_dir) / conversation_id / "submissions"
        
        if not submissions_dir.exists():
            return []
        
        records = []
        for submission_file in submissions_dir.glob("*.json"):
            try:
                with open(submission_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    
                    # æå–å…³é”®ä¿¡æ¯
                    # å¤„ç†æ—¶é—´æˆ³ï¼šå…¼å®¹ timestamp å’Œ graded_at å­—æ®µ
                    timestamp = data.get("timestamp") or data.get("graded_at")
                    if isinstance(timestamp, str) and timestamp:
                        # ISOæ ¼å¼è½¬æ—¶é—´æˆ³
                        from datetime import datetime
                        try:
                            dt = datetime.fromisoformat(timestamp)
                            submit_time = int(dt.timestamp() * 1000)
                        except:
                            submit_time = int(submission_file.stat().st_mtime * 1000)
                    elif isinstance(timestamp, (int, float)):
                        submit_time = int(timestamp)
                    else:
                        # ä½¿ç”¨æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                        submit_time = int(submission_file.stat().st_mtime * 1000)
                    
                    record = {
                        "id": submission_file.stem,  # æ–‡ä»¶åä½œä¸ºè®°å½•ID
                        "studentName": data.get("student_name", "æœªçŸ¥å­¦ç”Ÿ"),
                        "examName": f"è¯•å·æ‰¹æ”¹",
                        "score": round(data.get("average_score", 0)),
                        "maxScore": 100,
                        "submitTime": submit_time,
                        "pdfPath": data.get("pdf_path", None),  # PDFæŠ¥å‘Šè·¯å¾„
                        "details": data.get("per_question", []),
                        "recommendations": data.get("recommendations", []),
                        "knowledgeAnalysis": data.get("knowledgeAnalysis", {}),
                        "learningAdvice": data.get("learning_advice", None)
                    }
                    print(f"[DEBUG] è¯»å–è®°å½•: {record['studentName']}, åˆ†æ•°: {record['score']}, æ—¶é—´: {submit_time}")
                    records.append(record)
            except Exception as e:
                print(f"[âš ï¸ è¯»å–è®°å½•æ–‡ä»¶å¤±è´¥] {submission_file}: {e}")
                continue
        
        # æŒ‰æäº¤æ—¶é—´é™åºæ’åº
        records.sort(key=lambda x: x.get("submitTime", 0), reverse=True)
        print(f"[DEBUG] å…±æ‰¾åˆ° {len(records)} æ¡è®°å½•")
        for r in records:
            print(f"  - {r['studentName']}: {r['score']}åˆ†, æ—¶é—´:{r['submitTime']}")
        return records

    def delete_record(self, conversation_id: str, record_id: str):
        """
        åˆ é™¤æŒ‡å®šçš„å­¦ç”Ÿæ‰¹æ”¹è®°å½•
        åŒæ—¶åˆ é™¤ submissions/ã€learning_advice/ å’Œ grading_reports/ ç›®å½•ä¸‹çš„ç›¸å…³æ–‡ä»¶
        """
        # è¯»å–è®°å½•æ–‡ä»¶è·å–PDFè·¯å¾„
        submission_file = Path(config.settings.data_dir) / conversation_id / "submissions" / f"{record_id}.json"
        pdf_path_to_delete = None
        
        if submission_file.exists():
            try:
                with open(submission_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    pdf_path_to_delete = data.get("pdf_path")
            except:
                pass
            
            # åˆ é™¤æ‰¹æ”¹è®°å½•æ–‡ä»¶
            submission_file.unlink()
        else:
            raise ValueError(f"æœªæ‰¾åˆ°è®°å½•: {record_id}")
        
        # åˆ é™¤å­¦ä¹ å»ºè®®æ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        advice_file = Path(config.settings.data_dir) / conversation_id / "learning_advice" / f"{record_id}.json"
        if advice_file.exists():
            advice_file.unlink()
        
        # åˆ é™¤PDFæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if pdf_path_to_delete:
            pdf_file = Path(config.settings.data_dir) / pdf_path_to_delete
            if pdf_file.exists():
                pdf_file.unlink()
                print(f"[âœ… åˆ é™¤PDFæŠ¥å‘Š] {pdf_file}")
        
        print(f"[âœ… åˆ é™¤è®°å½•æˆåŠŸ] {record_id}")




