# -*- coding: utf-8 -*-
# ===========================================================
# æ–‡ä»¶ï¼šbackend/app/agents/agent_g_grader.py
# åŠŸèƒ½ï¼šAgent G - å¯¹ Agent E ç”Ÿæˆçš„é¢˜ç›®ä¸ç­”æ¡ˆè¿›è¡Œæ‰¹æ”¹ä¸è¯„åˆ†
# è¯´æ˜ï¼šè‡ªåŠ¨è¯»å– shared_state.generated_exam æˆ–ä»ç£ç›˜åŠ è½½ï¼Œä½¿ç”¨ LLM å¯¹æ¯é¢˜ç­”æ¡ˆ
#      ç»™å‡ºåˆ†æ•°ï¼ˆ0-100ï¼‰ã€å…³é”®æ€§åé¦ˆä¸æ”¹è¿›å»ºè®®ï¼Œä¿å­˜å¸¦è¯„åˆ†çš„é¢˜åº“ä¸æŠ¥å‘Šã€‚
# ===========================================================

import os
import re
import json
import asyncio
import aiohttp
from datetime import datetime
from dotenv import load_dotenv

from app.agents.shared_state import shared_state
from app.agents.database.question_bank_storage import load_question_bank, save_question_bank
from app.agents.models.quiz_models import QuestionBank
from app.agents.database.question_bank_storage import BASE_DATA_DIR


# Load environment
load_dotenv()
API_URL = os.getenv("LLM_BINDING_HOST", "https://api.siliconflow.cn/v1")
API_KEY = os.getenv("LLM_BINDING_API_KEY")
MODEL_NAME = os.getenv("LLM_MODEL", "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B")

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
}


async def async_grade_with_llm(session, q_idx: int, q_stem: str, q_answer: str, q_explanation: str = None):
    """
    è°ƒç”¨ LLM å¯¹å•é¢˜è¿›è¡Œè¯„åˆ†ä¸åé¦ˆï¼šè¾“å‡ºä¸¥æ ¼ JSON æ ¼å¼ï¼š
    {"score": int(0-100), "feedback": "...", "issues": ["..."], "suggestion": "..."}
    åœ¨å¤±è´¥æ—¶è¿”å›é»˜è®¤ä¸­æ€§è¯„ä¼°ã€‚
    """
    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯•é¢˜æ‰¹æ”¹ä¸“å®¶ã€‚è¯·æ ¹æ®é¢˜å¹²ä¸æ ‡å‡†ç­”æ¡ˆ/è§£æå¯¹è¯¥é¢˜çš„ç­”æ¡ˆè´¨é‡è¿›è¡Œè¯„åˆ†ä¸è¯Šæ–­ã€‚

è¦æ±‚ï¼š
- ç»™å‡ºä¸€ä¸ª 0-100 çš„æ•´æ•°åˆ†æ•°ï¼Œ100 è¡¨ç¤ºç­”æ¡ˆå®Œå¤‡ä¸”è§£é‡Šå……åˆ†ï¼›
- ç»™å‡ºç®€ç•¥åé¦ˆï¼ˆ1-2å¥ï¼‰ï¼Œæ ‡æ³¨å…³é”®ç¼ºé™·æˆ–äº®ç‚¹ï¼›
- åˆ—å‡º 0..n ä¸ªå…·ä½“é—®é¢˜ç‚¹ï¼ˆissuesï¼‰ï¼Œä¾‹å¦‚è¦ç‚¹ç¼ºå¤±ã€è®¡ç®—è¿‡ç¨‹é”™è¯¯ã€æ ¼å¼ä¸è§„èŒƒç­‰ï¼›
- ç»™å‡ºä¸€æ¡æ”¹è¿›å»ºè®®ï¼ˆsuggestionï¼‰ã€‚

è¾“å…¥é¢˜å¹²ï¼š""" + q_stem + """
è¾“å…¥ç­”æ¡ˆï¼š""" + (q_answer or "") + """
è§£æï¼ˆè‹¥æœ‰ï¼‰ï¼š""" + (q_explanation or "") + """

è¯·åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼Œä¸”ä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚
æ ¼å¼ç¤ºä¾‹ï¼š
{"score": 85, "feedback": "å›ç­”è¦ç‚¹å®Œæ•´ï¼Œä½†ç¼ºå°‘å…³é”®æ¨å¯¼æ­¥éª¤ã€‚", "issues": ["ç¼ºå°‘æ¨å¯¼æ­¥éª¤"], "suggestion": "è¡¥å……ç¬¬(b)å°é—®ä¸­çš„è®¡ç®—è¿‡ç¨‹å¹¶ç»™å‡ºä¸­é—´å…¬å¼ã€‚"}
"""

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è¯•é¢˜æ‰¹æ”¹ä¸ç‚¹è¯„åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 600,
        "temperature": 0.0,
    }

    for attempt in range(2):
        try:
            async with session.post(f"{API_URL}/chat/completions", headers=HEADERS, json=payload, timeout=120) as resp:
                res = await resp.json()
                content = res["choices"][0]["message"]["content"].strip()
                # æŠ½å–é¦–ä¸ª JSON å¯¹è±¡
                m = re.search(r"\{[\s\S]*\}", content)
                if m:
                    try:
                        parsed = json.loads(m.group(0))
                        # sanitize
                        score = int(parsed.get("score", 50))
                        feedback = parsed.get("feedback", "æ— åé¦ˆ")
                        issues = parsed.get("issues", []) or []
                        suggestion = parsed.get("suggestion", "")
                        return {"score": max(0, min(100, score)), "feedback": feedback, "issues": issues, "suggestion": suggestion}
                    except Exception:
                        # parsing error -> fallback try to parse more loosely
                        pass
        except Exception as e:
            if attempt == 0:
                await asyncio.sleep(1)
            else:
                # å¿½ç•¥å¹¶èµ°åå¤‡
                break

    # Fallback: æ ¹æ®ç­”æ¡ˆå†…å®¹åˆ¤æ–­
    # å¦‚æœç­”æ¡ˆä¸ºç©ºæˆ–æ˜æ˜¾ä¸å®Œæ•´ï¼Œç»™0åˆ†ï¼›å¦åˆ™ç»™30åˆ†å»ºè®®äººå·¥å¤æ ¸
    answer_text = (q_answer or "").strip()
    if not answer_text or len(answer_text) < 5:
        return {"score": 0, "feedback": "ç­”æ¡ˆä¸ºç©ºæˆ–è¿‡äºç®€çŸ­ï¼Œæ— æ³•è¯„åˆ†ã€‚", "issues": ["ç­”æ¡ˆç¼ºå¤±"], "suggestion": "è¯·æä¾›å®Œæ•´ç­”æ¡ˆã€‚"}
    return {"score": 30, "feedback": "æ¨¡å‹è°ƒç”¨å¤±è´¥ï¼Œæ— æ³•å‡†ç¡®è¯„åˆ†ï¼Œå»ºè®®äººå·¥å¤æ ¸ã€‚", "issues": ["è‡ªåŠ¨è¯„åˆ†å¤±è´¥"], "suggestion": "è¯·äººå·¥æ£€æŸ¥å…³é”®è®¡ç®—æˆ–è¦ç‚¹æ˜¯å¦é½å…¨ã€‚"}


async def async_grade_question_bank(qb: QuestionBank):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for idx, q in enumerate(qb.questions, start=1):
            # try to get explanation field if present
            explanation = getattr(q, "explanation", None)
            tasks.append(asyncio.create_task(async_grade_with_llm(session, idx, q.stem or "", q.answer or "", explanation)))

        results = await asyncio.gather(*tasks, return_exceptions=True)
    return results


def run_agent_g(conversation_id: str, expected_language: str = "English"):
    """
    Agent G ä¸»å‡½æ•°ï¼šå¯¹ Agent E ç”Ÿæˆçš„é¢˜åº“è¿›è¡Œé€é¢˜æ‰¹æ”¹å¹¶ç”Ÿæˆè´¨é‡æŠ¥å‘Šã€‚
    - ä» shared_state.generated_exam æˆ–ç£ç›˜åŠ è½½ç”Ÿæˆé¢˜åº“
    - å¹¶å‘è°ƒç”¨ LLM è·å–è¯„åˆ†ä¸åé¦ˆ
    - å°†è¯„åˆ†å†™å›é¢˜åº“å¯¹è±¡ï¼ˆä¸ºæ¯é¢˜å¢åŠ  `grade` å’Œ `grade_feedback` å­—æ®µï¼‰
    - ä¿å­˜å¸¦è¯„åˆ†çš„é¢˜åº“ï¼Œå¹¶å†™æ¦‚è¦æŠ¥å‘Šåˆ°åŒç›®å½•ä¸‹
    """
    print("ğŸ§© [Agent G] å¼€å§‹å¯¹ç”Ÿæˆé¢˜åº“è¿›è¡Œæ‰¹æ”¹ä¸è¯„åˆ†...")

    qb: QuestionBank = getattr(shared_state, "generated_exam", None)
    if qb is None or not getattr(qb, "questions", None):
        print("âš ï¸ shared_state.generated_exam ä¸ºç©ºï¼Œå°è¯•ä»ç£ç›˜åŠ è½½ã€‚")
        qb = load_question_bank(f"{conversation_id}_generated")

    if qb is None or not getattr(qb, "questions", None):
        print("âŒ æœªæ‰¾åˆ°å¯æ‰¹æ”¹çš„ç”Ÿæˆé¢˜åº“ï¼ŒAgent G ç»ˆæ­¢ã€‚")
        return None

    print(f"ğŸ‘‰ å‡†å¤‡å¯¹ {len(qb.questions)} é¢˜è¿›è¡Œæ‰¹æ”¹ï¼ˆconversation: {conversation_id}ï¼‰ã€‚")

    # è¿è¡Œå¼‚æ­¥æ‰¹æ”¹
    try:
        results = asyncio.run(async_grade_question_bank(qb))
    except Exception as e:
        print(f"[âŒ Agent G è°ƒåº¦å¼‚å¸¸] {type(e).__name__}: {e}")
        return None

    # å†™å›åˆ†æ•°ä¸åé¦ˆ
    total = 0
    count = 0
    per_question_reports = []
    for q, r in zip(qb.questions, results):
        if isinstance(r, Exception):
            report = {"score": 0, "feedback": "LLM å¼‚å¸¸ï¼Œæ— æ³•è¯„åˆ†ï¼Œå»ºè®®äººå·¥å¤æ ¸ã€‚", "issues": ["è¯„åˆ†å¤±è´¥"], "suggestion": "äººå·¥å¤æ ¸"}
            print(f"[âš ï¸ é¢˜ç›® {q.id} æ‰¹æ”¹å¤±è´¥ï¼Œç»™äºˆ0åˆ†] {r}")
        else:
            report = r

        # attach to question (best-effort, Question model may accept extra attrs)
        try:
            setattr(q, "grade", report.get("score"))
            setattr(q, "grade_feedback", report.get("feedback"))
            setattr(q, "grade_issues", report.get("issues"))
            setattr(q, "grade_suggestion", report.get("suggestion"))
        except Exception:
            pass

        per_question_reports.append({"id": getattr(q, "id", None), "score": report.get("score"), "feedback": report.get("feedback")})
        total += int(report.get("score", 50) or 0)
        count += 1

    avg_score = (total / max(count, 1)) if count else 0

    # ç”Ÿæˆæ•´ä½“è´¨é‡æŠ¥å‘Š
    quality_report = {
        "conversation_id": conversation_id,
        "graded_at": datetime.now().isoformat(),
        "question_count": count,
        "average_score": avg_score,
        "per_question": per_question_reports,
    }

    # å­˜å…¥å…±äº«çŠ¶æ€
    try:
        shared_state.quality_report = quality_report
        shared_state.generated_exam = qb
    except Exception:
        pass

    # ä¿å­˜å¸¦è¯„åˆ†çš„é¢˜åº“ï¼ˆä½¿ç”¨å·²æœ‰ä¿å­˜å‡½æ•°ï¼‰
    try:
        save_path = save_question_bank(f"{conversation_id}_graded", qb)
        # å†™å…¥å¹¶è¡Œçš„ JSON report æ–‡ä»¶æ—
        if save_path:
            report_path = os.path.splitext(save_path)[0] + "_grade_report.json"
            try:
                with open(report_path, "w", encoding="utf-8") as f:
                    json.dump(quality_report, f, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"[âš ï¸ æ— æ³•å†™å…¥æŠ¥å‘Šæ–‡ä»¶] {e}")

        print(f"âœ… æ‰¹æ”¹å®Œæˆå¹¶ä¿å­˜è‡³: {save_path}")
    except Exception as e:
        print(f"[âš ï¸ ä¿å­˜æ‰¹æ”¹ç»“æœå¤±è´¥] {e}")

    print(f"ğŸ“Š å¹³å‡åˆ†: {avg_score:.2f}ï¼ˆå…± {count} é¢˜ï¼‰")
    return quality_report


async def async_grade_student_answer(session, q: dict, student_answer: str):
    """ä½¿ç”¨ LLM å¯¹å•ä¸ªå­¦ç”Ÿç­”æ¡ˆè¿›è¡Œè¯„åˆ†ä¸åé¦ˆã€‚è¿”å› {score, feedback, issues}"""
    ref_answer = q.get("answer") or ""
    stem = q.get("stem") or ""

    prompt = f"""
ä½ æ˜¯ä¸¥æ ¼çš„é˜…å·è€å¸ˆã€‚ç»™å‡ºé¢˜å¹²ã€å‚è€ƒç­”æ¡ˆå’Œå­¦ç”Ÿç­”æ¡ˆï¼Œè¯·å¯¹å­¦ç”Ÿç­”æ¡ˆç»™å‡ºï¼š
1) scoreï¼ˆ0-100 æ•´æ•°ï¼‰ï¼Œ2) ç®€çŸ­è¯„è¯­ feedback, 3) åˆ—è¡¨ issuesï¼ˆè¦ç‚¹ç¼ºå¤±/é”™è¯¯/æ ¼å¼é—®é¢˜ç­‰ï¼‰ã€‚

é¢˜å¹²ï¼š{stem}
å‚è€ƒç­”æ¡ˆï¼š{ref_answer}
å­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}

åªè¾“å‡º JSON å¯¹è±¡ï¼š{{"score": int, "feedback": str, "issues": [str]}}
"""

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„é˜…å·è¯„åˆ†åŠ©æ‰‹ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 400,
        "temperature": 0.0
    }

    for attempt in range(2):
        try:
            async with session.post(f"{API_URL}/chat/completions", headers=HEADERS, json=payload, timeout=120) as resp:
                res = await resp.json()
                content = res["choices"][0]["message"]["content"].strip()
                m = re.search(r"\{[\s\S]*\}", content)
                if m:
                    try:
                        parsed = json.loads(m.group(0))
                        score = int(parsed.get("score", 0))
                        feedback = parsed.get("feedback", "")
                        issues = parsed.get("issues", []) or []
                        return {"score": max(0, min(100, score)), "feedback": feedback, "issues": issues}
                    except Exception:
                        pass
        except Exception:
            if attempt == 0:
                await asyncio.sleep(1)
            else:
                break

    # fallback: simple heuristic
    # exact match for single choice or short answer
    norm_student = (student_answer or "").strip().lower()
    norm_ref = (ref_answer or "").strip().lower()
    if not norm_student:
        return {"score": 0, "feedback": "æœªä½œç­”", "issues": ["ç©ºç­”"]}
    if norm_student == norm_ref:
        return {"score": 100, "feedback": "ç­”æ¡ˆå®Œå…¨åŒ¹é…å‚è€ƒç­”æ¡ˆ", "issues": []}
    # æ£€æŸ¥æ˜¯å¦åŒ…å«å‚è€ƒç­”æ¡ˆçš„å…³é”®éƒ¨åˆ†ï¼ˆç®€å•çš„éƒ¨åˆ†åŒ¹é…ï¼‰
    if norm_ref in norm_student or norm_student in norm_ref:
        return {"score": 80, "feedback": "ç­”æ¡ˆåŸºæœ¬æ­£ç¡®", "issues": []}
    # å®Œå…¨ä¸åŒ¹é…æ—¶ç»™0åˆ†
    return {"score": 0, "feedback": "ç­”æ¡ˆä¸å‚è€ƒç­”æ¡ˆä¸ç¬¦", "issues": ["ç­”æ¡ˆé”™è¯¯"]}


async def async_grade_submission(qb: QuestionBank, answers_map: dict):
    async with aiohttp.ClientSession() as session:
        tasks = []
        for q in qb.questions:
            qdict = q.model_dump() if hasattr(q, 'model_dump') else q.__dict__
            qid = qdict.get('id')
            student_ans = answers_map.get(qid, '')
            tasks.append(asyncio.create_task(async_grade_student_answer(session, qdict, student_ans)))

        results = await asyncio.gather(*tasks, return_exceptions=True)
    return results


def run_grade_student_submission(conversation_id: str, student_name: str, answers_map: dict):
    """
    å¯¹å­¦ç”Ÿæäº¤ï¼ˆanswers_map: {questionId: answer}ï¼‰è¿›è¡Œè¯„åˆ†ã€‚
    è¿”å› report dictï¼Œå¹¶ä¿å­˜åˆ° data/<conversation_id>/submissions/*.json
    """
    print(f"ğŸ§© [Agent G] å¼€å§‹å¯¹å­¦ç”Ÿæäº¤è¿›è¡Œè¯„åˆ†: conversation={conversation_id}, student={student_name}")

    qb = getattr(shared_state, 'generated_exam', None)
    if qb is None or not getattr(qb, 'questions', None):
        qb = load_question_bank(f"{conversation_id}_generated")

    if qb is None or not getattr(qb, 'questions', None):
        raise ValueError('æœªæ‰¾åˆ°ç”Ÿæˆé¢˜åº“ï¼Œæ— æ³•è¯„åˆ†')

    try:
        results = asyncio.run(async_grade_submission(qb, answers_map))
    except Exception as e:
        raise e

    per_q = []
    total = 0
    count = 0
    # knowledge aggregation
    kp_scores = {}
    kp_counts = {}

    for q, r in zip(qb.questions, results):
        qdict = q.model_dump() if hasattr(q, 'model_dump') else q.__dict__
        qid = qdict.get('id')
        student_ans = answers_map.get(qid, '')
        if isinstance(r, Exception):
            rec = {"score": 0, "feedback": 'è¯„åˆ†å¼‚å¸¸ï¼Œéœ€äººå·¥å¤æ ¸', "issues": []}
        else:
            rec = r

        per_q.append({
            "id": qid,
            "question_type": qdict.get('question_type'),
            "studentAnswer": student_ans,
            "score": rec.get('score'),
            "feedback": rec.get('feedback'),
            "issues": rec.get('issues', []),
            "knowledge_points": qdict.get('knowledge_points', [])
        })

        total += int(rec.get('score', 0) or 0)
        count += 1

        for kp in qdict.get('knowledge_points', []) or []:
            kp_scores[kp] = kp_scores.get(kp, 0) + rec.get('score', 0)
            kp_counts[kp] = kp_counts.get(kp, 0) + 1

    avg = total / max(count, 1)

    knowledgeAnalysis = {}
    for kp, s in kp_scores.items():
        cnt = kp_counts.get(kp, 1)
        mastery = (s / (cnt * 100)) if cnt else 0
        performance = 'ä¼˜ç§€' if mastery >= 0.8 else ('è‰¯å¥½' if mastery >= 0.6 else 'éœ€æ”¹è¿›')
        knowledgeAnalysis[kp] = {"masteryLevel": round(mastery, 3), "questionCount": cnt, "performance": performance}

    recommendations = []
    for kp, info in knowledgeAnalysis.items():
        if info['masteryLevel'] < 0.6:
            recommendations.append(f"çŸ¥è¯†ç‚¹ {kp} éœ€åŠ å¼ºï¼Œå»ºè®®é’ˆå¯¹ç›¸å…³é¢˜å‹åšä¸“é¡¹ç»ƒä¹ ")

    report = {
        "conversation_id": conversation_id,
        "student_name": student_name,
        "graded_at": datetime.now().isoformat(),
        "question_count": count,
        "average_score": avg,
        "per_question": per_q,
        "knowledgeAnalysis": knowledgeAnalysis,
        "recommendations": recommendations
    }

    # save report to disk
    try:
        subs_dir = os.path.join(BASE_DATA_DIR, conversation_id, 'submissions')
        os.makedirs(subs_dir, exist_ok=True)
        fname = f"submission_{student_name or 'anon'}_{datetime.now().strftime('%Y%m%d%H%M%S')}.json"
        fp = os.path.join(subs_dir, fname)
        with open(fp, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"[âš ï¸ ä¿å­˜å­¦ç”Ÿæäº¤æŠ¥å‘Šå¤±è´¥] {e}")

    return report


if __name__ == "__main__":
    # æ–¹ä¾¿è°ƒè¯•
    run_agent_g("test_a_text")
