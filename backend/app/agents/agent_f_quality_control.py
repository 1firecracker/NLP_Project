# -*- coding: utf-8 -*-
# ===========================================================
# æ–‡ä»¶ï¼šbackend/app/agents/agent_f_quality_control.py
# åŠŸèƒ½ï¼šAgent F - å‡ºé¢˜è´¨é‡æ§åˆ¶ã€è¯­è¨€ç»Ÿä¸€ã€çŸ¥è¯†ç‚¹è¦†ç›–ä¸é‡å¤æ£€æµ‹
# ===========================================================

import re
import json
import aiohttp
import asyncio
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from app.agents.shared_state import shared_state
from app.agents.database.question_bank_storage import save_question_bank, load_question_bank
from app.agents.models.quiz_models import Question, QuestionBank

API_URL = "https://api.siliconflow.cn/v1"
API_KEY = None
MODEL_NAME = "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B"

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
}

# ===========================================================
# åŸºç¡€å·¥å…·å‡½æ•°
# ===========================================================
def has_cjk(text: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", text))

def detect_language(text: str) -> str:
    if not text:
        return "unknown"
    cjk_ratio = len(re.findall(r"[\u4e00-\u9fff]", text)) / max(len(text), 1)
    return "Chinese" if cjk_ratio > 0.15 else "English"

# ===========================================================
# LLM ç¿»è¯‘æ¨¡å—
# ===========================================================
async def async_rewrite_to_english(session, q: dict):
    """è°ƒç”¨ LLM å°†é¢˜ç›®ç¿»è¯‘æˆçº¯è‹±æ–‡ç‰ˆæœ¬"""
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯­è¨€æ ¡å¯¹åŠ©æ‰‹ï¼Œè´Ÿè´£å°†è¯•é¢˜ç¿»è¯‘æˆçº¯è‹±æ–‡è¾“å‡ºï¼Œä¿æŒåŸå­—æ®µç»“æ„ã€‚"},
            {"role": "user", "content": f"è¯·å°†ä»¥ä¸‹JSONé¢˜ç›®ç¿»è¯‘æˆè‹±æ–‡ï¼Œä¸æ”¹å˜å­—æ®µç»“æ„ï¼š\n{json.dumps(q, ensure_ascii=False)}"}
        ],
        "max_tokens": 1000,
        "temperature": 0.3
    }
    try:
        async with session.post(f"{API_URL}/chat/completions", headers=HEADERS, json=payload, timeout=120) as resp:
            res = await resp.json()
            content = res["choices"][0]["message"]["content"]
            match = re.search(r"\{.*\}", content, re.S)
            if match:
                return json.loads(match.group(0))
    except Exception as e:
        print(f"[âš ï¸ ç¿»è¯‘å¤±è´¥] {e}")
    return q

# ===========================================================
# çŸ¥è¯†ç‚¹è¦†ç›–ç‡åˆ†æ
# ===========================================================
def _norm_kp(kp: str) -> str:
    """ç®€å•å½’ä¸€åŒ–ï¼šå»ç©ºç™½ã€å…¨è§’è½¬åŠè§’ã€ç»Ÿä¸€å°å†™"""
    if kp is None:
        return ""
    s = str(kp).strip().lower()
    # å¯æŒ‰éœ€æ‰©å±•ï¼šå…¨è§’è½¬åŠè§’ã€å»æ ‡ç‚¹ç­‰
    return s

def analyze_knowledge_coverage_binary(qb: QuestionBank):
    """
    äºŒå€¼è¦†ç›–ç‡ï¼š
    - ä» Agent C çš„ distribution_model['knowledge_point_distribution'] å–â€œæœŸæœ›çŸ¥è¯†ç‚¹é›†åˆâ€
    - ä»ç”Ÿæˆé¢˜åº“ä¸­æ±‡æ€»æ‰€æœ‰ knowledge_pointsï¼Œå–â€œå®é™…å‘½ä¸­é›†åˆâ€
    - è¦†ç›–ç‡ = å‘½ä¸­é›†åˆå¤§å° / æœŸæœ›é›†åˆå¤§å°
    - åŒæ—¶è¾“å‡ºæ¯ä¸ªçŸ¥è¯†ç‚¹æ˜¯å¦è¢«è¦†ç›–ï¼ˆ1/0ï¼‰ï¼Œä»¥åŠæœªè¦†ç›–æ¸…å•
    """
    dist_model = getattr(shared_state, "distribution_model", None)
    if not dist_model or "knowledge_point_distribution" not in dist_model:
        print("[âš ï¸ æ— æ³•è¿›è¡ŒçŸ¥è¯†ç‚¹è¦†ç›–åˆ†æï¼šæœªæ‰¾åˆ°åˆ†å¸ƒæ¨¡å‹æˆ– KP åˆ†å¸ƒ]")
        return {"coverage_rate": 0.0, "covered_map": {}, "missing": []}

    expected_kps_raw = list(dist_model["knowledge_point_distribution"].keys())
    expected_set = {_norm_kp(kp) for kp in expected_kps_raw if kp}

    # æ±‡æ€»ç”Ÿæˆé¢˜åº“çš„ KP
    actual_set = set()
    for q in qb.questions:
        for kp in (q.knowledge_points or []):
            actual_set.add(_norm_kp(kp))

    # é€ç‚¹äºŒå€¼å‘½ä¸­è¡¨
    covered_map = {}
    for kp in expected_kps_raw:
        nk = _norm_kp(kp)
        covered_map[kp] = 1 if nk in actual_set else 0

    hit = sum(covered_map.values())
    total = max(len(expected_set), 1)
    coverage_rate = hit / total

    # è¾“å‡ºæŠ¥å‘Š
    print("\nğŸ“Š [F] çŸ¥è¯†ç‚¹äºŒå€¼è¦†ç›–ç‡ï¼š")
    for kp in expected_kps_raw:
        print(f"  - {kp}: {covered_map[kp]}")

    missing = [kp for kp in expected_kps_raw if covered_map[kp] == 0]
    if missing:
        print(f"  âš ï¸ æœªè¦†ç›–çŸ¥è¯†ç‚¹ï¼ˆ{len(missing)}ï¼‰: {missing}")
    else:
        print("  âœ… æœŸæœ›çŸ¥è¯†ç‚¹å·²å…¨éƒ¨è¦†ç›–ã€‚")

    return {"coverage_rate": coverage_rate, "covered_map": covered_map, "missing": missing}

# ===========================================================
# é‡å¤åº¦æ£€æµ‹
# ===========================================================
def detect_duplicates(qb: QuestionBank, threshold=0.85):
    stems = [q.stem for q in qb.questions]
    vectorizer = TfidfVectorizer(stop_words="english")
    X = vectorizer.fit_transform(stems)
    sim_matrix = cosine_similarity(X)
    print("\nğŸ” [F] é¢˜å¹²é‡å¤åº¦æ£€æµ‹ï¼š")
    duplicates = []
    for i in range(len(stems)):
        for j in range(i + 1, len(stems)):
            if sim_matrix[i, j] > threshold:
                duplicates.append((qb.questions[i].id, qb.questions[j].id, sim_matrix[i, j]))
                print(f"  âš ï¸ Q{i+1:03d} ä¸ Q{j+1:03d} ç›¸ä¼¼åº¦ {sim_matrix[i,j]:.2f}")
    if not duplicates:
        print("  âœ… æœªå‘ç°é«˜åº¦ç›¸ä¼¼çš„é¢˜ç›®ã€‚")
    return duplicates

# ===========================================================
# å¼‚æ­¥ä¸»ä»»åŠ¡
# ===========================================================
async def async_quality_control(qb: QuestionBank, expected_lang="English"):
    async with aiohttp.ClientSession() as session:
        new_questions = []
        for q in qb.questions:
            q_dict = q.__dict__
            lang = detect_language(q.stem)
            if lang != expected_lang:
                print(f"[F] æ£€æµ‹åˆ°è¯­è¨€ä¸ä¸€è‡´ï¼š{q.id} ({lang} â†’ {expected_lang})ï¼Œå¼€å§‹è‡ªåŠ¨ç¿»è¯‘...")
                q_dict = await async_rewrite_to_english(session, q_dict)
            for field in ["stem", "answer", "difficulty", "knowledge_points", "question_type"]:
                if not q_dict.get(field):
                    print(f"[âš ï¸ ç¼ºå¤±å­—æ®µ] {q.id} â†’ {field}")
            new_questions.append(Question(**q_dict))
        return QuestionBank(questions=new_questions)

# ===========================================================
# å¯¹å¤–ä¸»å‡½æ•°
# ===========================================================
def run_agent_f(conversation_id: str, expected_language="English"):
    print("ğŸ§© [Agent F] å¼€å§‹è´¨é‡ã€è¯­è¨€ä¸ä¸€è‡´æ€§æ ¡å¯¹...")
    qb = getattr(shared_state, "generated_exam", None)
    if qb is None:
        print("âš ï¸ æœªæ‰¾åˆ°å†…å­˜ä¸­çš„é¢˜åº“ï¼Œå°è¯•ä»ç£ç›˜åŠ è½½ã€‚")
        qb = load_question_bank(f"{conversation_id}_generated")
    if qb is None:
        print("âŒ æ— å¯æ ¡å¯¹é¢˜åº“ã€‚")
        return None

    # Step 1: è¯­è¨€ & æ ¼å¼ç»Ÿä¸€
    new_qb = asyncio.run(async_quality_control(qb, expected_lang=expected_language))

    # Step 2: çŸ¥è¯†ç‚¹è¦†ç›–ç‡åˆ†æ
    cov = analyze_knowledge_coverage_binary(new_qb)

    # Step 3: é‡å¤åº¦æ£€æµ‹
    duplicate_report = detect_duplicates(new_qb, threshold=0.85)

    # Step 4: ä¿å­˜ç»“æœ
    save_path = save_question_bank(f"{conversation_id}_corrected", new_qb)
    print(f"\nâœ… Agent F æ ¡å¯¹å®Œæˆå¹¶ä¿å­˜è‡³: {save_path}")
    print(f"ğŸ“ˆ è¦†ç›–ç‡(äºŒå€¼): {cov['coverage_rate'] * 100:.2f}%")
    print(f"ğŸ”¸ æœªè¦†ç›–çŸ¥è¯†ç‚¹: {len(cov['missing'])}")
    print(f"ğŸ” æ½œåœ¨é‡å¤é¢˜: {len(duplicate_report)} å¯¹\n")
    return new_qb
