# -*- coding: utf-8 -*-
# ===========================================================
# æ–‡ä»¶ï¼šbackend/app/agents/agent_a_data_preparation.py
# åŠŸèƒ½ï¼šAgent A - Markdown æ ·å·æŠ½é¢˜ â†’ QuestionBank
# ===========================================================

import json
import os
import re
import time
from typing import List, Tuple

import requests
from openai import OpenAI  # type: ignore
from dotenv import load_dotenv  # type: ignore

from app.agents.shared_state import shared_state
from app.agents.database.question_bank_storage import save_question_bank
from app.agents.models.quiz_models import Question, QuestionBank, SubQuestion

# -----------------------------------------------------------
# ç¯å¢ƒé…ç½®
# -----------------------------------------------------------
load_dotenv()
API_URL = os.getenv("LLM_BINDING_HOST", "https://api.siliconflow.cn/v1")
API_KEY = os.getenv("LLM_BINDING_API_KEY")
MODEL_NAME = os.getenv("LLM_MODEL", "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B")
HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
}
OPENAI_CLIENT = OpenAI(api_key=API_KEY, base_url=API_URL) if OpenAI else None
BASE_DIR = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))

PROMPT_TEMPLATE = """ä½ æ˜¯ä¸€åâ€œè¯•é¢˜æŠ½å–ä¸ç»“æ„åŒ–åŠ©æ‰‹â€ã€‚è¾“å…¥æ˜¯ä¸€ä»½ Markdown æ ¼å¼çš„è¯•å·å†…å®¹ï¼Œé¢˜ç›®ç¼–å·ã€
åˆ†å€¼å’Œå­é—®æ ‡è®°å½¢å¼å¯èƒ½ä¸å®Œå…¨ç»Ÿä¸€ã€‚è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹ JSON æ¨¡æ¿è¾“å‡ºé¢˜ç›®æ•°ç»„ï¼Œå¹¶æ”¯æŒå­é—®é¢˜é€’å½’åµŒå¥—ï¼š
[
  {{
    "id": "é¢˜ç›®ç¼–å·ï¼ˆå¦‚ 1, 2, 3 æˆ– 1(a)ï¼‰",
    "stem": "é¢˜å¹²å…¨æ–‡ï¼ˆå»æ‰é¢˜å·ã€åˆ†å€¼æç¤ºï¼‰",
    "score": é¢˜ç›®æ€»åˆ†ï¼ˆæ•°å­—ï¼Œç¼ºå¤±å¡« 0ï¼‰,
    "question_type": "short_answer | calculation | multiple_choice | single_choice  | essay ",
    "sub_questions": [
        {{
            "label": "å­é—®æ ‡è®°ï¼ˆa/i ç­‰ï¼‰",
            "stem": "å­é—®é¢˜å†…å®¹",
            "score": å­é—®åˆ†å€¼ï¼ˆæ•°å­—ï¼Œç¼ºå¤±å¡« 0ï¼‰,
            "question_type": "å­é¢˜é¢˜å‹ï¼ŒåŒä¸Šå–å€¼èŒƒå›´",
            "sub_questions": [
                {{
                    "label": "å­é—®å†…çš„å­é—®æ ‡è®°ï¼ˆå¦‚ a-1/i - 1/1/2 ç­‰ï¼‰",
                    "stem": "æ›´ç»†ä¸€çº§çš„å­é—®å†…å®¹",
                    "score": å­é—®åˆ†å€¼ï¼ˆæ•°å­—ï¼Œç¼ºå¤±å¡« 0ï¼‰,
                    "question_type": "å­é¢˜é¢˜å‹ï¼ŒåŒä¸Šå–å€¼èŒƒå›´"
                }}
            ]
        }}
    ]
  }}
]
- å¦‚æœé¢˜ç›®æ²¡æœ‰å­é—®ï¼Œsub_questions è®¾ä¸ºç©ºæ•°ç»„ï¼›å¦‚æœå­é—®ä¸‹è¿˜æœ‰å­é—®ï¼Œç»§ç»­ä½¿ç”¨åŒæ ·ç»“æ„é€’å½’åµŒå¥—ã€‚
- è‹¥æ•´é¢˜æˆ–å­é—®ç¼ºå°‘åˆ†å€¼ï¼Œå¡« 0ï¼›é¢˜å‹æ— æ³•åˆ¤æ–­åˆ™å¡« otherã€‚
- ä¿ç•™ Markdown/LaTeX å…¬å¼å†…å®¹ã€‚
- ä»…è¾“å‡ºåˆæ³• JSONï¼Œä¸è¦æ·»åŠ é¢å¤–è§£é‡Šæˆ–ä»£ç å—æ ‡è®°ã€‚

è¯·å¤„ç†ä»¥ä¸‹ Markdownï¼š
<<<BEGIN_MARKDOWN
{markdown}
<<<END_MARKDOWN
"""

# -----------------------------------------------------------
# Markdown æ–‡ä»¶æ‰«æ
# -----------------------------------------------------------

def _scan_markdown_files(conversation_id: str, provided_paths: List[str] = None) -> List[str]:
    """
    æ”¶é›†ä¼šè¯ä¸‹æ‰€æœ‰ .md æ–‡ä»¶ã€‚
    """
    if provided_paths:
        sanitized = [
            p for p in provided_paths
            if p and p.strip().upper() != "__AUTO__"
        ]
        if sanitized:
            return [p for p in sanitized if p.lower().endswith(".md") and os.path.exists(p)]

    base_dir = os.path.join(BASE_DIR, "uploads", "exercises", conversation_id, "samples")
    detected = []
    if not os.path.exists(base_dir):
        return detected

    for root, _, files in os.walk(base_dir):
        for f in files:
            if f.lower().endswith(".md"):
                detected.append(os.path.join(root, f))
    return detected


# -----------------------------------------------------------
# LLM è°ƒç”¨
# -----------------------------------------------------------

def _extract_json_array(text: str):
    """
    ä» LLM è¾“å‡ºä¸­æå– JSON æ•°ç»„ï¼Œæ”¯æŒåµŒå¥—ç»“æ„ã€‚
    ä¼˜å…ˆå°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬ï¼Œå¤±è´¥åˆ™å°è¯•æå–ä»£ç å—æˆ–æ•°ç»„ç‰‡æ®µã€‚
    """
    if not text:
        return []
    
    text = text.strip()
    
    # 1. å°è¯•ç›´æ¥è§£ææ•´ä¸ªæ–‡æœ¬
    if text.startswith('['):
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass
    
    # 2. å°è¯•æå– ```json ... ``` æˆ– ``` ... ``` ä»£ç å—
    code_block_match = re.search(r'```(?:json)?\s*(\[[\s\S]*?\])\s*```', text)
    if code_block_match:
        try:
            return json.loads(code_block_match.group(1))
        except json.JSONDecodeError:
            pass
    
    # 3. å°è¯•æå–ç¬¬ä¸€ä¸ªå®Œæ•´çš„ JSON æ•°ç»„ï¼ˆæ”¯æŒåµŒå¥—ï¼‰
    # ä½¿ç”¨æ ˆåŒ¹é…æ‹¬å·ï¼Œæ‰¾åˆ°å®Œæ•´çš„ [ ... ] ç»“æ„
    start_idx = text.find('[')
    if start_idx == -1:
        return []
    
    bracket_count = 0
    in_string = False
    escape_next = False
    
    for i in range(start_idx, len(text)):
        char = text[i]
        
        if escape_next:
            escape_next = False
            continue
        
        if char == '\\':
            escape_next = True
            continue
        
        if char == '"':
            in_string = not in_string
            continue
        
        if not in_string:
            if char == '[':
                bracket_count += 1
            elif char == ']':
                bracket_count -= 1
                if bracket_count == 0:
                    # æ‰¾åˆ°å®Œæ•´æ•°ç»„
                    json_str = text[start_idx:i+1]
                    try:
                        return json.loads(json_str)
                    except json.JSONDecodeError:
                        pass
                    break
    
    return []


def _estimate_tokens(text: str) -> int:
    """ç²—ç•¥ä¼°ç®— token æ•°ï¼šä¸­æ–‡æŒ‰å­—ç¬¦ï¼Œè‹±æ–‡æŒ‰ç©ºæ ¼åˆ†è¯ï¼Œçº¦ 1.3 å€"""
    if not text:
        return 0
    # ç®€å•å¯å‘å¼ï¼šä¸­æ–‡å­—ç¬¦æ•° + è‹±æ–‡å•è¯æ•°
    import re
    cjk_count = len(re.findall(r'[\u4e00-\u9fff]', text))
    eng_words = len(re.findall(r'\b[a-zA-Z]+\b', text))
    return int((cjk_count + eng_words) * 1.3)



def extract_questions_via_llm(markdown_text: str, conversation_id: str, source_name: str) -> List[dict]:
    """
    è°ƒç”¨ LLM å°† Markdown è½¬ä¸ºç»“æ„åŒ–é¢˜ç›®åˆ—è¡¨ã€‚
    """
    if not markdown_text.strip():
        return []

    prompt = PROMPT_TEMPLATE.format(markdown=markdown_text)
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è¯•é¢˜æŠ½å–ä¸“å®¶ï¼Œä¸“é—¨ä» Markdown è¯•å·ä¸­å®šä½é¢˜ç›®ã€‚"},
            {"role": "user", "content": prompt},
        ],
        "temperature": 0.2,
        "max_tokens": 2000,
    }
    # payload["response_format"] = {"type": "json_object"}

    # ä¼°ç®—è¾“å…¥ token æ•°
    total_text = payload["messages"][0]["content"] + payload["messages"][1]["content"]
    est_tokens = _estimate_tokens(total_text)
    print(f"ğŸ§¾ LLM é¢„è®¡ tokens: {est_tokens}")

    for attempt in range(2):
        try:
            start_ts = time.time()
            if OPENAI_CLIENT is not None:
                print("open ai client sending successfully")
                response = OPENAI_CLIENT.chat.completions.create(
                    model=MODEL_NAME,
                    messages=payload["messages"],
                    temperature=payload["temperature"],
                    max_tokens=payload["max_tokens"],
                    # response_format=payload["response_format"],
                    extra_body={
                        "thinking_budget": 1
                    }
                )
                cost = time.time() - start_ts
                print(f"â±ï¸ LLM è¯·æ±‚è€—æ—¶: {cost:.2f}s")
                content = response.choices[0].message.content or ""
            else:
                resp = requests.post(
                    f"{API_URL}/chat/completions",
                    headers=HEADERS,
                    json=payload,
                    timeout=500,
                )
                cost = time.time() - start_ts
                print(f"â±ï¸ LLM è¯·æ±‚è€—æ—¶: {cost:.2f}s")
                resp.raise_for_status()
                data = resp.json()
                content = data["choices"][0]["message"]["content"]

            # ä¿å­˜åŸå§‹è¾“å‡º
            debug_dir = os.path.join(BASE_DIR, "data", conversation_id, "debug")
            os.makedirs(debug_dir, exist_ok=True)
            debug_file = os.path.join(debug_dir, f"agent_a_{source_name}_attempt{attempt+1}.txt")
            with open(debug_file, "w", encoding="utf-8") as dbg:
                dbg.write(content)
            print(f"ğŸ“ LLM åŸå§‹è¾“å‡ºå·²ä¿å­˜ï¼š{debug_file}")

            parsed = _extract_json_array(content)
            if parsed:
                return parsed
            print(f"[âš ï¸ LLM è¿”å›æ— æ³•è§£æ JSONï¼Œå°è¯•é‡è¯•] attempt={attempt+1}")
        except requests.RequestException as e:
            cost = time.time() - start_ts if 'start_ts' in locals() else 0.0
            print(f"â±ï¸ LLM è¯·æ±‚è€—æ—¶: {cost:.2f}s")
            print(f"[âš ï¸ LLM è¯·æ±‚å¤±è´¥ attempt={attempt+1}] {e}")
    return []


# -----------------------------------------------------------
# Question å¯¹è±¡æ„å»º
# -----------------------------------------------------------

def _parse_sub_questions(entries: List[dict]) -> List[SubQuestion]:
    """é€’å½’è§£æå­é—®é¢˜åˆ—è¡¨"""
    parsed: List[SubQuestion] = []
    if not isinstance(entries, list):
        return parsed

    for index, entry in enumerate(entries, 1):
        if not isinstance(entry, dict):
            continue
        label_raw = entry.get("label", "")
        label = str(label_raw).strip() if label_raw is not None else ""
        stem_raw = entry.get("stem", "")
        stem = str(stem_raw).strip() if stem_raw is not None else ""
        if not stem:
            continue
        score_value = entry.get("score", 0)
        if isinstance(score_value, (int, float)):
            score = int(score_value)
        else:
            score = 0
        qtype_raw = entry.get("question_type", "short_answer")
        qtype = str(qtype_raw).strip() if qtype_raw is not None else "short_answer"
        child_entries = entry.get("sub_questions", [])
        children = _parse_sub_questions(child_entries) if isinstance(child_entries, list) else []
        label_final = label if label else f"sub_{index}"
        parsed.append(
            SubQuestion(
                label=label_final,
                stem=stem,
                score=score,
                question_type=qtype or "short_answer",
                sub_questions=children,
            )
        )
    return parsed


def _convert_items_to_questions(items: List[dict], source_label: str) -> List[Question]:
    """
    å°† LLM è¿”å›çš„é¢˜ç›®åˆ—è¡¨è½¬æ¢ä¸º Question å¯¹è±¡ï¼Œä¿ç•™åµŒå¥—çš„ sub_questions ç»“æ„ã€‚
    """
    questions: List[Question] = []
    for idx, item in enumerate(items, 1):
        if not isinstance(item, dict):
            continue
        stem_raw = item.get("stem", "")
        stem = str(stem_raw).strip() if stem_raw is not None else ""
        if not stem:
            continue
        qid_raw = item.get("id", f"Q{idx:03d}")
        qid = str(qid_raw).strip() if qid_raw is not None else f"Q{idx:03d}"
        qtype_raw = item.get("question_type", "short_answer")
        qtype = str(qtype_raw).strip() if qtype_raw is not None else "short_answer"
        score_value = item.get("score", 0)
        if isinstance(score_value, (int, float)):
            score = int(score_value)
        else:
            score = 0
        sub_questions = _parse_sub_questions(item.get("sub_questions", []))

        tags = [f"source:{source_label}", f"score:{score}"]

        question = Question(
            id=qid if qid else f"Q{idx:03d}",
            stem=stem,
            answer="ï¼ˆå¾…è¡¥å……ï¼‰",
            difficulty="medium",
            knowledge_points=[],
            question_type=qtype or "short_answer",
            tags=tags,
            sub_questions=sub_questions,
        )
        questions.append(question)
    return questions


# -----------------------------------------------------------
# ä¸»æ‰§è¡Œå‡½æ•°
# -----------------------------------------------------------

def run_agent_a(conversation_id: str, file_paths: List[str] = None) -> QuestionBank:
    print(f"ğŸ§© [Agent A] Markdown æŠ½é¢˜å¼€å§‹ï¼Œä¼šè¯ID: {conversation_id}")

    md_files = _scan_markdown_files(conversation_id, file_paths)
    if not md_files:
        print(f"âš ï¸ ä¼šè¯ {conversation_id} æœªæ‰¾åˆ°ä»»ä½• .md æ ·å·ï¼Œè¿”å›ç©ºé¢˜åº“äº¤ç”± Agent E å¤„ç†ã€‚")
        qb = QuestionBank(questions=[], source_docs=[])
        shared_state.question_bank = qb
        shared_state.source_text = ""
        return qb

    print(f"ğŸ‘‰ å·²å‘ç° {len(md_files)} ä¸ª Markdown æ ·å·ï¼š")
    for f in md_files:
        print(f"   - {f}")

    aggregated_texts = []
    all_questions: List[Question] = []
    for md_path in md_files:
        try:
            with open(md_path, "r", encoding="utf-8") as f:
                content = f.read()
        except Exception as e:
            print(f"[âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶] {md_path}: {e}")
            continue

        aggregated_texts.append(f"\n\n# Source: {md_path}\n{content}")
        source_name = os.path.splitext(os.path.basename(md_path))[0]
        relative_name = os.path.relpath(md_path, os.path.join(BASE_DIR, "uploads"))

        llm_items = extract_questions_via_llm(content, conversation_id, source_name)
        if not llm_items:
            print(f"[âš ï¸ LLM æœªä» {md_path} ä¸­è§£æåˆ°é¢˜ç›®]")
            continue
        relative_name = os.path.relpath(md_path, os.path.join(BASE_DIR, "uploads"))
        questions = _convert_items_to_questions(llm_items, relative_name)
        all_questions.extend(questions)
        print(f"âœ… {md_path} è§£æå¾—åˆ° {len(questions)} é“é¢˜")

    if not all_questions:
        print("âš ï¸ æ‰€æœ‰ Markdown æ ·å·å‡æœªæˆåŠŸæŠ½å–é¢˜ç›®ï¼Œè¿”å›ç©ºé¢˜åº“äº¤ç”± Agent E å¤„ç†ã€‚")
        qb = QuestionBank(questions=[], source_docs=md_files)
        shared_state.question_bank = qb
        shared_state.source_text = "\n".join(aggregated_texts)
        return qb

    qb = QuestionBank(questions=all_questions, source_docs=md_files)
    shared_state.question_bank = qb
    shared_state.source_text = "\n".join(aggregated_texts)

    save_path = save_question_bank(conversation_id, qb)
    print(f"âœ… Agent A å®Œæˆï¼Œå…±æŠ½å– {len(all_questions)} é¢˜ã€‚é¢˜åº“å·²ä¿å­˜ï¼š{save_path}")
    return qb
