"""
ä¸ºåŒ…å«è¡¨æ ¼çš„é¢˜ç›®ç”Ÿæˆç­”æ¡ˆï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
"""
import json
import os
import requests
from pathlib import Path
from dotenv import load_dotenv
import re
import time

load_dotenv()

API_URL = os.getenv("LLM_BINDING_HOST", "https://api.siliconflow.cn/v1")
API_KEY = os.getenv("LLM_BINDING_API_KEY")
MODEL_NAME = os.getenv("LLM_MODEL", "deepseek-ai/DeepSeek-R1-0528-Qwen3-8B")

HEADERS = {
    "Authorization": f"Bearer {API_KEY}",
    "Content-Type": "application/json",
}

def html_table_to_markdown(html_text):
    """ç®€å•çš„HTMLè¡¨æ ¼è½¬Markdown"""
    if not html_text or '<table' not in html_text.lower():
        return html_text
    
    result = html_text
    table_pattern = re.compile(r'<table[^>]*>(.*?)</table>', re.DOTALL | re.IGNORECASE)
    
    for table_match in table_pattern.finditer(html_text):
        table_html = table_match.group(0)
        table_content = table_match.group(1)
        
        rows = re.findall(r'<tr[^>]*>(.*?)</tr>', table_content, re.DOTALL | re.IGNORECASE)
        if not rows:
            continue
        
        markdown_rows = []
        for i, row in enumerate(rows):
            cells = re.findall(r'<t[hd][^>]*>(.*?)</t[hd]>', row, re.DOTALL | re.IGNORECASE)
            if cells:
                clean_cells = []
                for cell in cells:
                    cell_text = re.sub(r'<[^>]+>', '', cell)
                    cell_text = ' '.join(cell_text.split())
                    clean_cells.append(cell_text)
                
                markdown_rows.append('| ' + ' | '.join(clean_cells) + ' |')
                
                if i == 0:
                    markdown_rows.append('| ' + ' | '.join(['---'] * len(clean_cells)) + ' |')
        
        if markdown_rows:
            markdown_table = '\n' + '\n'.join(markdown_rows) + '\n'
            result = result.replace(table_html, markdown_table)
    
    return result


def generate_answer(question_id, stem):
    """ä½¿ç”¨LLMä¸ºé¢˜ç›®ç”Ÿæˆç­”æ¡ˆ"""
    
    # å°†HTMLè¡¨æ ¼è½¬ä¸ºMarkdown
    stem_for_llm = html_table_to_markdown(stem)
    
    prompt = f"""è¯·ä¸ºä»¥ä¸‹é¢˜ç›®æä¾›è¯¦ç»†çš„ç­”æ¡ˆã€‚é¢˜ç›®åŒ…å«è¡¨æ ¼æ•°æ®ï¼Œè¯·ä»”ç»†åˆ†æè¡¨æ ¼ä¸­çš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ã€‚

é¢˜ç›®ï¼š
{stem_for_llm}

è¦æ±‚ï¼š
1. å¦‚æœé¢˜ç›®æœ‰å¤šä¸ªå­é—®é¢˜(a)(b)(c)ç­‰ï¼Œè¯·åˆ†åˆ«ä½œç­”
2. å¯¹äºè®¡ç®—é¢˜ï¼Œç»™å‡ºè®¡ç®—æ­¥éª¤å’Œæœ€ç»ˆç»“æœ
3. å¯¹äºåˆ†æé¢˜ï¼Œç»™å‡ºæ¸…æ™°çš„åˆ†ææ€è·¯å’Œç»“è®º
4. ç­”æ¡ˆè¦ç®€æ´æ˜ç¡®ï¼Œé‡ç‚¹çªå‡ºå…³é”®æ­¥éª¤å’Œç»“è®º
5. ä½¿ç”¨ä¸­æ–‡ä½œç­”

è¯·ç›´æ¥è¾“å‡ºç­”æ¡ˆï¼Œä¸è¦é‡å¤é¢˜ç›®ï¼š
"""
    
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": "ä½ æ˜¯ä¸€åæ•°æ®æŒ–æ˜å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„ä¸“å®¶ï¼Œæ“…é•¿è§£ç­”ç®—æ³•ã€æ•°å­¦è®¡ç®—å’Œæ•°æ®åˆ†æç›¸å…³çš„é—®é¢˜ã€‚"},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 2000,
        "temperature": 0.3,
    }
    
    try:
        print(f"[â†’] æ­£åœ¨ä¸ºé¢˜ç›® {question_id} ç”Ÿæˆç­”æ¡ˆ...")
        response = requests.post(
            f"{API_URL}/chat/completions",
            headers=HEADERS,
            json=payload,
            timeout=300  # å¢åŠ åˆ° 5 åˆ†é’Ÿï¼Œå› ä¸º DeepSeek-R1 éœ€è¦é•¿æ—¶é—´æ€è€ƒ
        )
        
        if response.status_code != 200:
            print(f"âŒ HTTPé”™è¯¯ {response.status_code}: {response.text}")
            return f"ï¼ˆç”Ÿæˆå¤±è´¥ï¼šHTTP {response.status_code}ï¼‰"
        
        res = response.json()
        
        if "error" in res:
            print(f"âŒ APIé”™è¯¯: {res['error']}")
            return "ï¼ˆç”Ÿæˆå¤±è´¥ï¼šAPIé”™è¯¯ï¼‰"
        
        if "choices" not in res or len(res["choices"]) == 0:
            print(f"âŒ å“åº”æ ¼å¼é”™è¯¯: {res}")
            return "ï¼ˆç”Ÿæˆå¤±è´¥ï¼šå“åº”æ ¼å¼é”™è¯¯ï¼‰"
        
        answer = res["choices"][0]["message"]["content"].strip()
        print(f"âœ… é¢˜ç›® {question_id} ç­”æ¡ˆå·²ç”Ÿæˆ (é•¿åº¦: {len(answer)})")
        return answer
        
    except Exception as e:
        import traceback
        print(f"âŒ é¢˜ç›® {question_id} ç”Ÿæˆç­”æ¡ˆå¤±è´¥:")
        print(traceback.format_exc())
        return "ï¼ˆç”Ÿæˆå¤±è´¥ï¼‰"


def main():
    qb_file = Path(r"C:\Users\19668\Desktop\workspace\NLP_Project-yhx_test\backend\data\c84f70ce-4f86-4a90-b1d8-158e9bdd0fc0\quiz\question_bank.json")
    
    # è¯»å–é¢˜åº“
    with open(qb_file, 'r', encoding='utf-8') as f:
        qb_data = json.load(f)
    
    # æ‰¾å‡ºéœ€è¦ç”Ÿæˆç­”æ¡ˆçš„é¢˜ç›®ï¼ˆåŒ…æ‹¬å¤±è´¥çš„ï¼‰
    questions_to_answer = []
    for q in qb_data['question_bank']['questions']:
        answer = q.get('answer', '')
        if '<table' in q.get('stem', '') and (answer == 'ï¼ˆå¾…è¡¥å……ï¼‰' or 'ç”Ÿæˆå¤±è´¥' in answer):
            questions_to_answer.append(q)
    
    if not questions_to_answer:
        print("æ²¡æœ‰æ‰¾åˆ°éœ€è¦ç”Ÿæˆç­”æ¡ˆçš„è¡¨æ ¼é¢˜")
        return
    
    print(f"\næ‰¾åˆ° {len(questions_to_answer)} é“éœ€è¦ç”Ÿæˆç­”æ¡ˆçš„è¡¨æ ¼é¢˜\n")
    
    # é€ä¸ªç”Ÿæˆç­”æ¡ˆ
    for i, q in enumerate(questions_to_answer, 1):
        print(f"\n[{i}/{len(questions_to_answer)}] å¤„ç†é¢˜ç›® {q['id']}...")
        answer = generate_answer(q['id'], q['stem'])
        q['answer'] = answer
        
        # é¿å…APIé™æµ
        if i < len(questions_to_answer):
            time.sleep(3)
    
    # å¤‡ä»½åŸæ–‡ä»¶
    import shutil
    backup_file = qb_file.with_suffix('.json.backup5')
    shutil.copy(qb_file, backup_file)
    print(f"\nğŸ“¦ åŸæ–‡ä»¶å·²å¤‡ä»½åˆ°: {backup_file}")
    
    # ä¿å­˜æ›´æ–°åçš„é¢˜åº“
    with open(qb_file, 'w', encoding='utf-8') as f:
        json.dump(qb_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… é¢˜åº“å·²æ›´æ–°ï¼Œå…±ä¸º {len(questions_to_answer)} é“é¢˜ç›®ç”Ÿæˆäº†ç­”æ¡ˆ")
    print("è¯·åˆ·æ–°å‰ç«¯é¡µé¢æŸ¥çœ‹")


if __name__ == "__main__":
    main()
